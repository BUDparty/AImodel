{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BUDparty/AImodel/blob/main/Retrieval_Augmented_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predibase + LlamaIndex: Building a RAG System\n",
        "The following walkthrough shows you how to use Predibase-hosted LLMs with LlamaIndex to build a RAG system.\n",
        "\n",
        "There are a few pieces required to build a RAG system:\n",
        "\n",
        "1. **LLM provider**\n",
        "* Predibase is the LLM provider here. We can serve base LLMs and/or fine-tuned LLMs for whatever generative task you have.\n",
        "2. **Embedding Model**\n",
        "* This model generates embeddings for the data that you are storing in your Vector Store.\n",
        "* In this example you have the option of using a local HuggingFace embedding model, or OpenAI's embedding model.\n",
        "** Note: You need to have an OpenAI account with funds and an API token to use the OpenAI embedding model.\n",
        "* In the near future, you will be able to train and deploy your own embedding models using Predibase\n",
        "3. **Vector Store**\n",
        "* This is where we store the embedded data that we want to retrieve later at query time\n",
        "* In this example we will use Pinecone for our Vector Store"
      ],
      "metadata": {
        "id": "BL5sMoWxJfu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "### Predibase\n",
        "If you don't have a Predibase account already, sign up for a free trial here\n",
        "Once you've logged in, navigate to Settings > My profile\n",
        "Generate a new API token\n",
        "Copy the API token and paste in the first setup cell below\n",
        "\n",
        "### OpenAI (Optional)\n",
        "If you don't have an OpenAI account already, sign up here\n",
        "Navigate to OpenAI's API keys page\n",
        "If you have not already, generate an API key\n",
        "Copy the API key and paste in the second setup cell below\n",
        "\n",
        "### Pinecone\n",
        "If you don't have a Pinecone account already, they have a free tier available for trial\n",
        "Navigate to the API Keys page\n",
        "If you have not already, generate an API key"
      ],
      "metadata": {
        "id": "UtHN1J4eKQ4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0: Setup"
      ],
      "metadata": {
        "id": "Oun5tuVkKasM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41MeQOyLRlUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import openai\n",
        "import pinecone\n",
        "\n",
        "from llama_index import ServiceContext, StorageContext, SimpleDirectoryReader, VectorStoreIndex, set_global_service_context\n",
        "from llama_index.llms import PredibaseLLM\n",
        "from llama_index.embeddings import HuggingFaceEmbedding, OpenAIEmbedding\n",
        "from llama_index.vector_stores import PineconeVectorStore\n",
        "\n",
        "os.environ[\"PREDIBASE_API_TOKEN\"] = \"YOUR API TOKEN HERE\""
      ],
      "metadata": {
        "id": "6MvQv15wKcp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is only required if you'll be using an OpenAI embedding model.\n",
        "\n"
      ],
      "metadata": {
        "id": "b20YGBihKjYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR API TOKEN HERE\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "PoYUrQC6KjuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setting up the Predibase LLM\n",
        "There a few parameters to keep in mind while setting up your Predibase LLM:\n",
        "\n",
        "1. model_name: This must be an LLM currently deployed in your Predibase environment.\n",
        "* Any of models shown in the LLM query view dropdown are valid options.\n",
        "* If you are running Predibase in a VPC, you'll need to deploy an LLM first.\n",
        "2. temperature: Controls the randomness of your model responses.\n",
        "A higher value will give the model more creative leeway\n",
        "A lower value will give a more reproducible and consistent response\n",
        "3. max_new_tokens: Controls the number of tokens the model can produce."
      ],
      "metadata": {
        "id": "Km7AZnYtKnBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Predibase LLM\n",
        "predibase_llm = PredibaseLLM(model_name=\"llama-2-13b-chat\", temperature=0.1, max_new_tokens=512)"
      ],
      "metadata": {
        "id": "vd8sWEwtKoCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Set up Embedding model\n",
        "If you are using a local HuggingFace embedding model, you can use the following code to set up your embedding model:"
      ],
      "metadata": {
        "id": "kPypQEvMK3Uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loads BAAI/bge-small-en-v1.5\n",
        "hf_embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ],
      "metadata": {
        "id": "jAEq8ow8K4xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using OpenAI's embedding model, you can use the following code to set up your embedding model:\n",
        "\n"
      ],
      "metadata": {
        "id": "ftD_nQv4K6Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loads text-embedding-ada-002 OpenAI embedding model - uncomment and run for the OpenAI option\n",
        "openai_embed_model = OpenAIEmbedding()"
      ],
      "metadata": {
        "id": "Vpb4LXc8K7Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now with our embedding model set up, we will create the service context that will be used to query the LLM and embed our data/queries."
      ],
      "metadata": {
        "id": "UjZ9cpAlK873"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ServiceContext with our Predibase LLM and chosen embedding model\n",
        "ctx = ServiceContext.from_defaults(llm=predibase_llm, embed_model=hf_embed_model)\n",
        "\n",
        "# Set the Predibase LLM ServiceContext to the default\n",
        "set_global_service_context(ctx)"
      ],
      "metadata": {
        "id": "g5pHIS3KK-n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Set up Vector Store\n",
        "As mentioned before, we'll be using Pinecone for this example. Pinecone has a free tier that you can use to try out this example. You can also swap out any other Vector Store supported by LlamaIndex."
      ],
      "metadata": {
        "id": "Tux9R9avLALH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pinecone and create index\n",
        "pinecone.init(api_key=\"YOUR API TOKEN HERE\", environment=\"gcp-starter\")"
      ],
      "metadata": {
        "id": "u7KS4wwDLCLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using the HuggingFace embedding model, you can use the following code to set up your Vector Store:"
      ],
      "metadata": {
        "id": "6tkjJtVaLHBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HF Index - Compatible with local HF embedding model output dimensions\n",
        "pinecone.create_index(\"predibase-demo-hf\", dimension=384, metric=\"euclidean\", pod_type=\"p1\")"
      ],
      "metadata": {
        "id": "r7JphjoBLFPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using the OpenAI embedding model, you can use the following code to set up your Vector Store:\n",
        "\n",
        "Note: You need to have OpenAI set up and configured for this option. If you do not have an OpenAI API key, we recommend you go with the HuggingFace Index option above."
      ],
      "metadata": {
        "id": "ZdS_UTRlLKP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI Index - Compatible with OpenAI embedding model (text-embedding-ada-002) output dimensions\n",
        "pinecone.create_index(\"predibase-demo-openai\", dimension=1536, metric=\"euclidean\", pod_type=\"p1\")"
      ],
      "metadata": {
        "id": "-6FNHJADLKgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll select our index, create the storage context, and index our documents!"
      ],
      "metadata": {
        "id": "cQ-74wEDLMSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct vector store and custom storage context\n",
        "pincone_vector_store = PineconeVectorStore(pinecone.Index(\"predibase-demo-hf\"))\n",
        "pinecone_storage_context = StorageContext.from_defaults(vector_store=pincone_vector_store)\n",
        "\n",
        "# Load in the documents you want to index\n",
        "documents = SimpleDirectoryReader(\"/Users/connor/Documents/Projects/datasets/huffington_post_pdfs/\").load_data()"
      ],
      "metadata": {
        "id": "pKRv89HKLOBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Set up index\n",
        "Here we create the index so that any query you make will pull the relevant context from your Vector Store."
      ],
      "metadata": {
        "id": "0oUoAcA9LSpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, storage_context=pinecone_storage_context)"
      ],
      "metadata": {
        "id": "R9KagcVWLUe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Querying the LLM with RAG\n",
        "Now that we've set up our index, we can ask questions over the documents and Predibase + LlamaIndex will search for the relevant context and provide a response to your question within said context."
      ],
      "metadata": {
        "id": "jRE52UbELavA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup query engine\n",
        "predibase_query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "ux-HK0q0LdJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can ask questions over our documents!"
      ],
      "metadata": {
        "id": "8m4390iZLcrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = predibase_query_engine.query(\"INSERT QUERY HERE\")"
      ],
      "metadata": {
        "id": "MYZK21OULfva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the response to your query, you can pass the response variable to a print statement. Otherwise, you can pass the response object around your system to finish setting up your RAG solution."
      ],
      "metadata": {
        "id": "2NXJ9Z4NLjNp"
      }
    }
  ]
}